{"paragraphs":[{"title":"Required Dependencies","text":"%dep\n\nz.load(\"org.apache.spark:spark-streaming_2.10:jar:1.6.2\")\nz.load(\"org.apache.spark:spark-streaming-kafka_2.10:jar:1.6.2\")\nz.load(\"org.apache.kafka:kafka_2.10:jar:0.10.0.1\")\nz.load(\"com.yammer.metrics:metrics-core:jar:2.2.0\")\nz.load(\"com.101tec:zkclient:jar:0.9\")\n\nz.load(\"C:/WorkspaceMain/xap-kafka-ie/parent/common/target/common-1.0.0-SNAPSHOT.jar\")\n","dateUpdated":"2017-06-27T10:49:43-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497289626662_-667815406","id":"20160830-145728_362255206","result":{"code":"SUCCESS","type":"TEXT","msg":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@49623b16\n"},"dateCreated":"2017-06-12T01:47:06-0400","dateStarted":"2017-06-27T10:49:43-0400","dateFinished":"2017-06-27T10:49:52-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:110","focus":true},{"title":"Read from Kafka Topic using Direct Stream","text":"%spark\n\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport _root_.kafka.serializer.StringDecoder\nimport org.apache.spark.streaming.kafka.KafkaUtils\n\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.spark.streaming.kafka.OffsetRange\n\nval ssc = new StreamingContext(sc, Seconds(15))\nssc.checkpoint(\"c:/ie-training/tmp\")\n\nvar topics = Set(\"priceFeed\") \n\nval kafkaParams = Map(\n    \"metadata.broker.list\" -> \"localhost:9092\",\n    \"auto.offset.reset\" -> \"smallest\"\n)","dateUpdated":"2017-06-27T10:49:55-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497289626663_-668200155","id":"20160830-170427_1816087646","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.streaming.{Seconds, StreamingContext}\nimport _root_.kafka.serializer.StringDecoder\nimport org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.spark.streaming.kafka.OffsetRange\nssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@2d301efd\ntopics: scala.collection.immutable.Set[String] = Set(priceFeed)\nkafkaParams: scala.collection.immutable.Map[String,String] = Map(metadata.broker.list -> localhost:9092, auto.offset.reset -> smallest)\n"},"dateCreated":"2017-06-12T01:47:06-0400","dateStarted":"2017-06-27T10:49:55-0400","dateFinished":"2017-06-27T10:50:11-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:111","focus":true},{"text":"%spark\n\nimport org.common.PriceFeed\nimport org.common.CustomDeserializer\nimport kafka.serializer.DefaultDecoder\n\n//val messages = KafkaUtils.createDirectStream[String, PriceFeed, StringDecoder, CustomDeserializer](ssc, kafkaParams, topics)\nval messages = KafkaUtils.createDirectStream[String, Array[Byte], StringDecoder, CustomDeserializer](ssc, kafkaParams, topics)\n//val messages = KafkaUtils.createDirectStream[String,  Array[Byte], StringDecoder, DefaultDecoder](ssc, kafkaParams, topics) \n//messages.foreachRDD(message => println(message.toString())) // output is KafkaRDD[0] at createDirectStream at <console>:51 KafkaRDD[1] at createDirectStream at <console>:51 ...\nmessages.print() // output is (null,PriceFeed [id=A1^1497450614721^14, symbol=A0, price=1.0]) \n\n/*messages.foreachRDD(rdd => \n    if (!rdd.isEmpty) {\n        val count = rdd.count.toInt\n        println(\"count received \" + count) // displays count received 3966 \n        rdd.take(1).foreach(println)    // this will print all RDD such as (null,PriceFeed [id=A1^1497450614721^14, symbol=A0, price=1.0]) \n        val priceFeedRDD = rdd.values\n        priceFeedRDD.take(2).foreach(println) // returns PriceFeed [id=A1^1497450614721^14, symbol=A0, price=1.0] \"no key\"\n        //priceFeedRDD.saveToGrid()\n    }\n)*/\n\nssc.start ","dateUpdated":"2017-06-27T10:51:01-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497453580703_1296867157","id":"20170614-111940_344094011","result":{"code":"ERROR","type":"TEXT","msg":"import org.common.PriceFeed\nimport org.common.CustomDeserializer\nimport kafka.serializer.DefaultDecoder\n<console>:54: error: type arguments [String,Array[Byte],kafka.serializer.StringDecoder,org.common.CustomDeserializer] conform to the bounds of none of the overloaded alternatives of\n value createDirectStream: [K, V, KD <: kafka.serializer.Decoder[K], VD <: kafka.serializer.Decoder[V]](jssc: org.apache.spark.streaming.api.java.JavaStreamingContext, keyClass: Class[K], valueClass: Class[V], keyDecoderClass: Class[KD], valueDecoderClass: Class[VD], kafkaParams: java.util.Map[String,String], topics: java.util.Set[String])org.apache.spark.streaming.api.java.JavaPairInputDStream[K,V] <and> [K, V, KD <: kafka.serializer.Decoder[K], VD <: kafka.serializer.Decoder[V]](ssc: org.apache.spark.streaming.StreamingContext, kafkaParams: Map[String,String], topics: Set[String])(implicit evidence$19: scala.reflect.ClassTag[K], implicit evidence$20: scala.reflect.ClassTag[V], implicit evidence$21: scala.reflect.ClassTag[KD], implicit evidence$22: scala.reflect.ClassTag[VD])org.apache.spark.streaming.dstream.InputDStream[(K, V)]\n         val messages = KafkaUtils.createDirectStream[String, Array[Byte], StringDecoder, CustomDeserializer](ssc, kafkaParams, topics)\n                                   ^\n"},"dateCreated":"2017-06-14T11:19:40-0400","dateStarted":"2017-06-27T10:51:01-0400","dateFinished":"2017-06-27T10:51:02-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:112","focus":true},{"text":"%spark\n\n//ssc.stop(stopSparkContext=false, stopGracefully=true)\n","dateUpdated":"2017-06-27T10:45:54-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497289626664_-670123900","id":"20160830-170845_1145510546","dateCreated":"2017-06-12T01:47:06-0400","dateStarted":"2017-06-27T10:48:45-0400","dateFinished":"2017-06-27T10:48:45-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:113","errorMessage":""},{"dateUpdated":"2017-06-27T10:45:54-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497289626664_-670123900","id":"20160830-165856_475845132","dateCreated":"2017-06-12T01:47:06-0400","dateStarted":"2017-06-27T10:48:45-0400","dateFinished":"2017-06-27T10:48:45-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:114","errorMessage":""}],"name":"Spark Streaming Kafka Access","id":"2CKMDXZBD","angularObjects":{"2BY3CWM32:shared_process":[],"2C1697SXS:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}