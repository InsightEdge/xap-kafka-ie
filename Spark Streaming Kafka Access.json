{"paragraphs":[{"title":"Required Dependencies","text":"%dep\n\nz.load(\"org.apache.spark:spark-streaming_2.10:jar:1.6.2\")\nz.load(\"org.apache.spark:spark-streaming-kafka_2.10:jar:1.6.2\")\nz.load(\"org.apache.kafka:kafka_2.10:jar:0.10.0.1\")\nz.load(\"com.yammer.metrics:metrics-core:jar:2.2.0\")\nz.load(\"com.101tec:zkclient:jar:0.9\")\n\nz.load(\"C:/WorkspaceMain/xapmirror-kafka/parent/common/target/common-1.0.0-SNAPSHOT.jar\")\n","dateUpdated":"2017-06-15T09:51:01-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497289626662_-667815406","id":"20160830-145728_362255206","result":{"code":"SUCCESS","type":"TEXT","msg":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@4b8b4b9\n"},"dateCreated":"2017-06-12T01:47:06-0400","dateStarted":"2017-06-15T09:51:01-0400","dateFinished":"2017-06-15T09:51:09-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:352"},{"text":"import org.insightedge.scala.annotation._\nimport scala.beans.{BeanProperty, BooleanBeanProperty}\n\n@SpaceClass\ncase class PriceFeed(\n   @BeanProperty\n   @SpaceId\n   var id: String,\n   \n   @BeanProperty\n   var symbol: String,\n   \n   @BeanProperty\n   var price: Float\n) {\n   def this() = this(null, null, -1)\n}","dateUpdated":"2017-06-15T09:51:01-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497453762509_1018402543","id":"20170614-112242_465759418","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.insightedge.scala.annotation._\nimport scala.beans.{BeanProperty, BooleanBeanProperty}\ndefined class PriceFeed\n"},"dateCreated":"2017-06-14T11:22:42-0400","dateStarted":"2017-06-15T09:51:03-0400","dateFinished":"2017-06-15T09:53:09-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:353"},{"title":"Read from Kafka Topic using Direct Stream","text":"%spark\n\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport _root_.kafka.serializer.StringDecoder\nimport org.apache.spark.streaming.kafka.KafkaUtils\n\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.spark.streaming.kafka.OffsetRange\n\nval ssc = new StreamingContext(sc, Seconds(15))\nssc.checkpoint(\"c:/ie-training/tmp\")\n\nvar topics = Set(\"priceFeed\") \n\nval kafkaParams = Map(\n    \"metadata.broker.list\" -> \"localhost:9092\",\n    \"auto.offset.reset\" -> \"smallest\"\n)","dateUpdated":"2017-06-15T09:51:01-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497289626663_-668200155","id":"20160830-170427_1816087646","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.streaming.{Seconds, StreamingContext}\nimport _root_.kafka.serializer.StringDecoder\nimport org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.spark.streaming.kafka.OffsetRange\nssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@694b629b\ntopics: scala.collection.immutable.Set[String] = Set(priceFeed)\nkafkaParams: scala.collection.immutable.Map[String,String] = Map(metadata.broker.list -> localhost:9092, auto.offset.reset -> smallest)\n"},"dateCreated":"2017-06-12T01:47:06-0400","dateStarted":"2017-06-15T09:51:09-0400","dateFinished":"2017-06-15T09:53:14-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:354"},{"text":"%spark\n\nval messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics) \n//messages.foreachRDD(message => println(message.toString())) // output is KafkaRDD[0] at createDirectStream at <console>:51 KafkaRDD[1] at createDirectStream at <console>:51 ...\n//messages.print() // output is (null,PriceFeed [id=A1^1497450614721^14, symbol=A0, price=1.0]) \n\nmessages.foreachRDD(rdd => \n    if (!rdd.isEmpty) {\n        val count = rdd.count.toInt\n        println(\"count received \" + count) // displays count received 3966 \n        rdd.take(1).foreach(println)    // this will print all RDD such as (null,PriceFeed [id=A1^1497450614721^14, symbol=A0, price=1.0]) \n        val priceFeedRDD = rdd.values\n        priceFeedRDD.take(1).foreach(println) // returns PriceFeed [id=A1^1497450614721^14, symbol=A0, price=1.0] \"no key\"\n        priceFeedRDD.saveToGrid()\n    }\n)\n\nssc.start ","dateUpdated":"2017-06-15T15:07:14-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497453580703_1296867157","id":"20170614-111940_344094011","result":{"code":"SUCCESS","type":"TEXT","msg":"messages: org.apache.spark.streaming.dstream.InputDStream[(String, String)] = org.apache.spark.streaming.kafka.DirectKafkaInputDStream@4fcc906d\n"},"dateCreated":"2017-06-14T11:19:40-0400","dateStarted":"2017-06-15T09:53:10-0400","dateFinished":"2017-06-15T09:53:16-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:355","focus":true},{"text":"%spark\n\n//ssc.stop(stopSparkContext=false, stopGracefully=true)\n","dateUpdated":"2017-06-15T09:51:01-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497289626664_-670123900","id":"20160830-170845_1145510546","dateCreated":"2017-06-12T01:47:06-0400","dateStarted":"2017-06-15T09:53:14-0400","dateFinished":"2017-06-15T09:53:16-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:356","result":{"code":"SUCCESS","type":"TEXT","msg":""}},{"dateUpdated":"2017-06-15T09:51:01-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1497289626664_-670123900","id":"20160830-165856_475845132","dateCreated":"2017-06-12T01:47:06-0400","dateStarted":"2017-06-15T09:53:16-0400","dateFinished":"2017-06-15T09:53:16-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:357","result":{"code":"SUCCESS","type":"TEXT","msg":""}}],"name":"Spark Streaming Kafka Access","id":"2CKMDXZBD","angularObjects":{"2BY3CWM32:shared_process":[],"2C1697SXS:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}